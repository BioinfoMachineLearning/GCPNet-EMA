program: src/train.py
method: grid
name: sweep_gcpnet_ema_2
metric: # NOTE: does not matter, as we are using sweep to run the experiment
  goal: minimize
  name: val/loss

parameters:
  callbacks:
    value: default

  data:
    value: ema

  data.batch_size:
    value: 16

  data.num_workers:
    value: 4

  extras.enforce_tags:
    value: False

  logger:
    value: wandb

  logger.wandb.entity:
    value: bml-lab

  logger.wandb.group:
    value: "Sweep-GCPNet-EMA"

  logger.wandb.project:
    value: "GCPNet-EMA"

  logger.wandb.tags:
    value: "gcpnetema"

  model:
    value: gcpnet_ema

  # NOTE: we want to load the pre-trained encoder weights
  model.model_cfg.finetune.encoder.load_weights:
    value: True

  # NOTE: we do not want to freeze the encoder or decoder
  model.model_cfg.finetune.encoder.freeze:
    value: False

  model.model_cfg.finetune.decoder.freeze:
    value: False

  model.optimizer.lr:
    values: [1e-5, 3e-5, 1e-4, 3e-4]

  model.optimizer.weight_decay:
    values: [1e-6, 1e-5]

  model.model_cfg.decoder.graph_regression.dropout:
    value: 0.0

  model.model_cfg.decoder.graph_regression.hidden_dim:
    value: [512, 512, 512]

  model.model_cfg.decoder.graph_regression.activations:
    value: ["relu", "relu", "relu", "none"]

  model.compile:
    value: False

  seed:
    value: 959

  trainer:
    value: default

  trainer.min_epochs:
    value: 1

  trainer.max_epochs:
    value: 1000

  test:
    value: True

  create_unverified_ssl_context:
    value: True

command:
  - ${env}
  - HYDRA_FULL_ERROR=1
  - WANDB_START_METHOD=thread
  - TRANSFORMERS_CACHE=/data/Transformers_Cache_Dir
  - /data/Repositories/Lab_Repositories/GCPNet-EMA/GCPNet-EMA/bin/python
  - ${program}
  - ${args_no_hyphens}
